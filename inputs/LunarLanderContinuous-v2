--batch_size
128
--latent_size
14
--activation
relu
--wae
--action_discretizer
--action_encoder_temperature
0.3333333333333333
--adam_beta_1
0
--adam_beta_2
0.999
--global_gradient_penalty_scale_factor
20
--global_network_layers
256
--latent_policy_temperature
0.5
--learning_rate
3e-4
--local_transition_loss_regularizer_scale_factor
50
--n_critic
15
--squared_wasserstein
False
--state_encoder_temperature
0.6
--state_prior_temperature
0.75
--steady_state_wasserstein_regularizer_scale_factor
100
--wasserstein_learning_rate
3e-4
--encoder_temperature_decay_rate
0.
--prior_temperature_decay_rate
0.
--number_of_discrete_actions
3
--parallel_env
8
--state_encoder_type
deterministic
--environment
LunarLanderContinuous-v2
--policy_environment
LunarLanderContinuous-v2
--env_suite
suite_gym
--policy_path=reinforcement_learning/saves/LunarLanderContinuous-v2/sac_policy
--checkpoint=True
--evaluation_window_size=0
--local_losses_evaluation
--local_losses_evaluation_steps
34000
--local_losses_replay_buffer_size
100000
--max_steps
1000000

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a48e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'reverb' Reverb is not installed on your system, meaning prioritized experience replay cannot be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 17:52:54.469084: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2022-08-09 17:52:54.469110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: florentdelgrange-MS-7C56\n",
      "2022-08-09 17:52:54.469114: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: florentdelgrange-MS-7C56\n",
      "2022-08-09 17:52:54.469170: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.203.3\n",
      "2022-08-09 17:52:54.469185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.172.1\n",
      "2022-08-09 17:52:54.469188: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 450.172.1 does not match DSO version 450.203.3 -- cannot find working devices in this configuration\n",
      "2022-08-09 17:52:54.469346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.insert(0, path + '/..')\n",
    "\n",
    "import base64\n",
    "import IPython\n",
    "import importlib\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "from tf_agents.environments import suite_gym, suite_dm_control, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment, FlattenObservationsWrapper\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "from tf_agents.trajectories import time_step as ts, policy_step, trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import TFPolicy\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from reinforcement_learning import labeling_functions\n",
    "import reinforcement_learning.environments\n",
    "from reinforcement_learning.environments import EnvironmentLoader, perturbed_env\n",
    "from reinforcement_learning.metrics import AverageDiscountedReturnMetric\n",
    "from policies.saved_policy import SavedTFPolicy\n",
    "from policies.epsilon_mimic import EpsilonMimicPolicy\n",
    "from policies.latent_policy import LatentPolicyOverRealStateAndActionSpaces\n",
    "\n",
    "from verification import model, local_losses, binary_latent_space\n",
    "from verification.value_iteration import value_iteration\n",
    "from verification.local_losses import compute_values_from_initial_distribution, PolicyDecorator\n",
    "from util.io.dataset_generator import ergodic_batched_labeling_function, is_reset_state\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from tf_agents.typing.types import Float, Bool\n",
    "\n",
    "from util.io import video\n",
    "import wasserstein_mdp\n",
    "\n",
    "# set seed\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45fc489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state_space(py_env):\n",
    "    print(\"state space shape:\", py_env.observation_spec().shape)\n",
    "    try:\n",
    "        print(\"state space max values:\", py_env.observation_spec().maximum)\n",
    "        print(\"state space min values:\", py_env.observation_spec().minimum)\n",
    "    except AttributeError as e:\n",
    "        pass\n",
    "\n",
    "def display_action_space(py_env):\n",
    "    if py_env.action_spec().dtype in [np.int64, np.int32]:\n",
    "        print(\"discrete action space\")\n",
    "        print(\"number of discrete actions:\", py_env.action_spec().maximum + 1)\n",
    "    else:\n",
    "        print(\"continuous action space\")\n",
    "        print(\"action space shape:\", py_env.action_spec().shape)\n",
    "        print(\"action space max values:\", py_env.action_spec().maximum)\n",
    "        print(\"action space min values:\", py_env.action_spec().minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eabcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.io.dataset_generator import is_reset_state\n",
    "from verification.local_losses import PolicyDecorator\n",
    "\n",
    "@tf.function\n",
    "def get_p_init(\n",
    "    wae_mdp,\n",
    "    original_state,\n",
    "    latent_transition_fn,\n",
    "    environment_name,\n",
    "):\n",
    "    latent_state_space = binary_latent_space(wae_mdp.latent_state_size)\n",
    "    is_reset_state_test_fn = lambda latent_state: is_reset_state(latent_state, wae_mdp.atomic_prop_dims)\n",
    "    original_reset_state = tf.tile(tf.zeros_like(original_state[:1, ...]), [tf.shape(latent_state_space)[0], 1])\n",
    "    reset_state = wae_mdp.state_embedding_function(\n",
    "        original_reset_state,\n",
    "        ergodic_batched_labeling_function(\n",
    "            labeling_functions[environment_name]\n",
    "        )(original_reset_state))\n",
    "    reset_state = tf.cast(reset_state, tf.float32)\n",
    "    \n",
    "    latent_action_space = tf.one_hot(\n",
    "        indices=tf.range(wae_mdp.number_of_discrete_actions),\n",
    "        depth=tf.cast(wae_mdp.number_of_discrete_actions, tf.int32),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(\n",
    "        tf.transpose(\n",
    "            PolicyDecorator(wae_mdp.get_latent_policy(action_dtype=tf.int64))(\n",
    "                reset_state\n",
    "            ).probs_parameter()\n",
    "        ) * tf.map_fn(\n",
    "            fn=lambda latent_action: latent_transition_fn(\n",
    "                reset_state,\n",
    "                tf.tile(tf.expand_dims(latent_action, 0), [tf.shape(latent_state_space)[0], 1]),\n",
    "            ).prob(\n",
    "                tf.cast(latent_state_space, tf.float32),\n",
    "                full_latent_state_space=True),\n",
    "            elems=latent_action_space),\n",
    "        axis=0) * (1. - tf.cast(is_reset_state_test_fn(latent_state_space), tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c398068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_until_T_values(\n",
    "    C_fn: Callable[[Float], Bool],\n",
    "    T_fn: Callable[[Float], Bool],\n",
    "    transition_matrix: Float,\n",
    "    latent_state_size: int,\n",
    "    A: int,\n",
    "    latent_policy: TFPolicy,\n",
    "    gamma: Float = 0.99,\n",
    "    transition_to_T_reward: Optional[Float] = None,\n",
    ") -> Float:\n",
    "    \n",
    "    S = tf.pow(2, latent_state_size)\n",
    "    state_space = binary_latent_space(latent_state_size, dtype=tf.float32)\n",
    "    \n",
    "    # make absorbing Â¬C and T\n",
    "    absorbing_states = lambda latent_state: tf.math.logical_or(\n",
    "        tf.math.logical_not(C_fn(latent_state)),\n",
    "        T_fn(latent_state))\n",
    "    \n",
    "    # reward of 1 when transitioning to T;\n",
    "    # set it to the input values if provided\n",
    "    reward_objective = tf.ones(\n",
    "        shape=(S, A, S),\n",
    "    ) * tf.cast(T_fn(state_space), tf.float32)\n",
    "    if transition_to_T_reward is not None:\n",
    "        reward_objective *= transition_to_T_reward\n",
    "    \n",
    "    policy_probs = PolicyDecorator(\n",
    "        latent_policy\n",
    "    )(state_space).probs_parameter()\n",
    "    \n",
    "    values = value_iteration(\n",
    "        latent_state_size=latent_state_size,\n",
    "        num_actions=A,\n",
    "        transition_fn=transition_matrix,\n",
    "        reward_fn=reward_objective,\n",
    "        gamma=gamma,\n",
    "        policy_probs=policy_probs,\n",
    "        epsilon=1e-6,\n",
    "        v_init=tf.zeros(S, dtype=tf.float32),\n",
    "        episodic_return=True,\n",
    "        is_reset_state_test_fn=absorbing_states,\n",
    "        error_type='absolute',\n",
    "        transition_matrix=transition_matrix,\n",
    "        reward_matrix=reward_objective,)\n",
    "    \n",
    "    # set the values of the target states to either one or the input values if provided\n",
    "    if transition_to_T_reward is None:\n",
    "        values = values + tf.cast(T_fn(state_space), tf.float32)\n",
    "    else:\n",
    "        values = values + (tf.cast(T_fn(state_space), tf.float32) * transition_to_T_reward)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b7e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_C_then_T_values(\n",
    "    C_fn: Callable[[Float], Bool],\n",
    "    T_fn: Callable[[Float], Bool],\n",
    "    transition_matrix: Float,\n",
    "    latent_state_size: int,\n",
    "    A: int,\n",
    "    latent_policy: TFPolicy,\n",
    "    gamma: Float = 0.99,\n",
    ") -> Float:\n",
    "    \n",
    "    S = tf.pow(2, latent_state_size)\n",
    "    state_space = binary_latent_space(latent_state_size, dtype=tf.float32)\n",
    "\n",
    "    C = C_fn(state_space)\n",
    "    T = T_fn(state_space)\n",
    "    all_states = tf.ones(shape=(S, A, S))\n",
    "    \n",
    "    # detect when the agent transitions from the C to T\n",
    "    # set C-state rows to 1 \n",
    "    from_C = tf.transpose(all_states * tf.cast(C, tf.float32))\n",
    "    # set T-state columns to 1 \n",
    "    to_T = all_states * tf.cast(T, tf.float32)\n",
    "    C_to_T_transitions = from_C * to_T\n",
    "    \n",
    "    # create the MDP augmented by a new absorbing state where C-states transition\n",
    "    # to instead of transitioning to T-states\n",
    "    #\n",
    "    # get the probability of transitioning from C to T\n",
    "    C_to_T_probs = tf.reduce_sum(transition_matrix * C_to_T_transitions, axis=-1)\n",
    "\n",
    "    # deviate the transitions from C to a new absorbing state\n",
    "    augmented_transition_matrix = tf.concat(\n",
    "        # set the probabilities of transitioning from C to T to 0.\n",
    "        [transition_matrix * (1. - C_to_T_transitions),\n",
    "         # set the transition probabilities to the absorbing state to those\n",
    "         # of transitioning to T\n",
    "         tf.expand_dims(C_to_T_probs, axis=-1)],\n",
    "        axis=-1)\n",
    "    # create a new sink state\n",
    "    sink_state_probs = tf.concat([\n",
    "            tf.zeros(shape=(1, A, S)),\n",
    "            tf.ones(shape=(1, A, 1))\n",
    "        ], axis=-1)\n",
    "    # add this sink state to the transition matrix of the augmented MDP\n",
    "    augmented_transition_matrix = tf.concat([\n",
    "        augmented_transition_matrix,\n",
    "        sink_state_probs,\n",
    "    ], axis=0)\n",
    "\n",
    "    # enable some random actions for the sink state\n",
    "    policy_probs = PolicyDecorator(\n",
    "        latent_policy\n",
    "    )(state_space).probs_parameter()\n",
    "    policy_probs = tf.concat([\n",
    "        policy_probs,\n",
    "        tf.pow(\n",
    "            tf.cast(A, tf.float32), -1.\n",
    "        ) * tf.ones(shape=(1, A))\n",
    "    ], axis=0)\n",
    "    \n",
    "    # reward of 1 when transitioning to the sink state\n",
    "    reward_objective = tf.concat([\n",
    "        tf.zeros(shape=(S, A, S)),\n",
    "        # add a last column full of ones\n",
    "        tf.ones(shape=(S, A, 1))\n",
    "    ], axis=-1)\n",
    "    reward_objective = tf.concat([\n",
    "        # add a last row full of zeros\n",
    "        reward_objective,\n",
    "        tf.zeros(shape=(1, A, S + 1))\n",
    "    ], axis=0)\n",
    "    \n",
    "    return value_iteration(\n",
    "        latent_state_size=latent_state_size,\n",
    "        num_actions=A,\n",
    "        transition_fn=augmented_transition_matrix,\n",
    "        reward_fn=reward_objective,\n",
    "        gamma=gamma,\n",
    "        policy_probs=policy_probs,\n",
    "        epsilon=1e-6,\n",
    "        v_init=tf.zeros(S + 1, dtype=tf.float32),\n",
    "        episodic_return=False,\n",
    "        error_type='absolute',\n",
    "        transition_matrix=augmented_transition_matrix,\n",
    "        reward_matrix=reward_objective,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77b93b",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8d4ccf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': '<wasserstein_mdp.WassersteinMarkovDecisionProcess object at 0x2addb46fe040>', 'state_shape': '(4,)', 'action_shape': '(2,)', 'reward_shape': '(1,)', 'label_shape': '(2,)', 'discretize_action_space': 'False', 'state_encoder_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='state_encoder_network_base')\", 'action_decoder_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='action_decoder_network_base')\", 'transition_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='transition_network_base')\", 'reward_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='reward_network_base')\", 'decoder_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='state_decoder_network_base')\", 'latent_policy_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='discrete_policy_network_base')\", 'steady_state_lipschitz_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='steady_state_network_base')\", 'transition_loss_lipschitz_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='transition_loss_network_base')\", 'latent_state_size': '9', 'number_of_discrete_actions': '16', 'action_encoder_network': \"ModelArchitecture(hidden_units=[64, 64, 64], activation='tanh', name='action_encoder_network_base')\", 'state_encoder_pre_processing_network': 'None', 'state_decoder_pre_processing_network': 'None', 'time_stacked_states': 'False', 'state_encoder_temperature': '0.3333333333333333', 'state_prior_temperature': '0.3333333333333333', 'action_encoder_temperature': '0.99', 'latent_policy_temperature': '0.6666666666666666', 'wasserstein_regularizer_scale_factor': 'WassersteinRegularizerScaleFactor(global_scaling=10.0, global_gradient_penalty_multiplier=20.0, steady_state_scaling=75.0, steady_state_gradient_penalty_multiplier=None, local_transition_loss_scaling=75.0, local_transition_loss_gradient_penalty_multiplier=None)', 'encoder_temperature_decay_rate': '1e-06', 'prior_temperature_decay_rate': '2e-06', 'reset_state_label': 'True', 'autoencoder_optimizer': 'None', 'wasserstein_regularizer_optimizer': 'None', 'entropy_regularizer_scale_factor': '0.0', 'entropy_regularizer_decay_rate': '0.0', 'entropy_regularizer_scale_factor_min_value': '0.0', 'importance_sampling_exponent': '0.4', 'importance_sampling_exponent_growth_rate': '1e-05', 'time_stacked_lstm_units': '128', 'reward_bounds': 'None', 'latent_stationary_network': 'None', 'action_entropy_regularizer_scaling': '0.0', 'enforce_upper_bound': 'False', 'squared_wasserstein': 'False', 'n_critic': '5', 'trainable_prior': 'False', 'state_encoder_type': 'EncodingType.DETERMINISTIC', 'policy_based_decoding': 'False', 'deterministic_state_embedding': 'True', 'state_encoder_softclipping': 'False', 'args': '()', 'kwargs': \"{'evaluation_window_size': 5}\", '__class__': \"<class 'wasserstein_mdp.WassersteinMarkovDecisionProcess'>\", 'eval_policy': '200.0', 'local_reward_loss': '0.0037750935', 'local_transition_loss': '0.40456417', 'training_step': '120000'}\n",
      "Model: \"state_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " state (InputLayer)          [(None, 4)]               0         \n",
      "                                                                 \n",
      " state_encoder_body (Sequent  (None, 60)               8380      \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 6)                 366       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,746\n",
      "Trainable params: 8,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "No action encoder\n",
      "Model: \"autoregressive_transition_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " logistic_layer_input (InputLay  [(None, 11)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " sequential_logistic_distributi  (None, 9)           0           ['logistic_layer_input[0][0]']   \n",
      " on_layer (Sequential)                                                                            \n",
      "                                                                                                  \n",
      " autoregressive_transform (Auto  ((None, 9),         11757       ['sequential_logistic_distributio\n",
      " regressiveTransform)            (None, 9))                      n_layer[0][0]',                  \n",
      "                                                                  'logistic_layer_input[0][0]']   \n",
      "                                                                                                  \n",
      " autoregressive_network_34 (Aut  (None, 9, 1)        11756       []                               \n",
      " oregressiveNetwork)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,757\n",
      "Trainable params: 11,756\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"latent_stationary_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " logistic_layer_input (Input  [(None, 0)]              0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " sequential_logistic_distrib  (None, 3)                0         \n",
      " ution_layer (Sequential)                                        \n",
      "                                                                 \n",
      " autoregressive_transform (A  ((None, 3),              8772      \n",
      " utoregressiveTransform)      (None, 3))                         \n",
      "                                                                 \n",
      " autoregressive_network_35 (  (None, 3, 1)             8771      \n",
      " AutoregressiveNetwork)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,778\n",
      "Trainable params: 8,771\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "Model: \"latent_policy_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_state (InputLayer)   [(None, 9)]               0         \n",
      "                                                                 \n",
      " discrete_policy_network_bas  (None, 64)               8960      \n",
      " e (Sequential)                                                  \n",
      "                                                                 \n",
      " latent_policy_categorical_l  (None, 2)                130       \n",
      " ogits (Dense)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,090\n",
      "Trainable params: 9,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"reward_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 9)]         0           []                               \n",
      "                                                                                                  \n",
      " reward_function_input (Concate  (None, 20)          0           ['latent_state[0][0]',           \n",
      " nate)                                                            'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " reward_network_base (Sequentia  (None, 64)          9664        ['reward_function_input[0][0]']  \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " reward_network_raw_output (Den  (None, 1)           65          ['reward_network_base[0][0]']    \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " reward (Reshape)               (None, 1)            0           ['reward_network_raw_output[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,729\n",
      "Trainable params: 9,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"state_reconstruction_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " next_latent_state (InputLay  [(None, 9)]              0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " state_decoder_network_base   (None, 64)               8960      \n",
      " (Sequential)                                                    \n",
      "                                                                 \n",
      " state_decoder (Sequential)  (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,220\n",
      "Trainable params: 9,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"steady_state_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 9)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 20)           0           ['latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " steady_state_network_base (Seq  (None, 64)          9664        ['concatenate_34[0][0]']         \n",
      " uential)                                                                                         \n",
      "                                                                                                  \n",
      " steady_state_lipschitz_network  (None, 1)           65          ['steady_state_network_base[0][0]\n",
      " _output (Dense)                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,729\n",
      "Trainable params: 9,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"transition_loss_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " state (InputLayer)             [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_state (InputLayer)      [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 9)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 24)           0           ['state[0][0]',                  \n",
      "                                                                  'action[0][0]',                 \n",
      "                                                                  'latent_state[0][0]',           \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " transition_loss_network_base (  (None, 64)          9920        ['concatenate_35[0][0]']         \n",
      " Sequential)                                                                                      \n",
      "                                                                                                  \n",
      " transition_loss_lipschitz_netw  (None, 1)           65          ['transition_loss_network_base[0]\n",
      " ork_output (Dense)                                              [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,985\n",
      "Trainable params: 9,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WAE-MDP loaded\n"
     ]
    }
   ],
   "source": [
    "wae_model_path = 'saved_models/experiments/CartPole-v0/model/'\n",
    "\n",
    "with open(os.path.join(wae_model_path, 'model_infos.json'), 'r') as f:\n",
    "    wae_data = json.load(f)\n",
    "    print(wae_data)\n",
    "\n",
    "wae_mdp = wasserstein_mdp.load(wae_model_path)\n",
    "print(\"WAE-MDP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "81cd8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE-MDP at training step 120000\n",
      "Size of the latent state space: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"WAE-MDP at training step {:d}\".format(eval(wae_data['training_step'])))\n",
    "print(\"Size of the latent state space: {:d}\".format(2 ** wae_mdp.latent_state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d818b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local reward loss: 0.0038\n",
      "Local transition loss: 0.4\n",
      "Transition/reward model generation\n",
      "Time to generate the model: 0.72 sec\n"
     ]
    }
   ],
   "source": [
    "with suite_gym.load('CartPole-v0') as py_env:\n",
    "    py_env.reset()\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "\n",
    "_latent_transition_fn = lambda latent_state, latent_action: \\\n",
    "        wae_mdp.discrete_latent_transition(\n",
    "            tf.cast(latent_state, tf.float32),\n",
    "            tf.cast(latent_action, tf.float32))\n",
    "\n",
    "print('Local reward loss: {:.2g}'.format(eval(wae_data['local_reward_loss'])))\n",
    "print('Local transition loss: {:.2g}'.format(eval(wae_data['local_transition_loss'])))\n",
    "\n",
    "print('Transition/reward model generation')\n",
    "start = time.time()\n",
    "\n",
    "#  write the transition/reward functions to tensors,\n",
    "#  to formally check the values in an efficient way\n",
    "latent_transition_fn = model.TransitionFunctionCopy(\n",
    "    num_states=tf.cast(tf.pow(2, wae_mdp.latent_state_size), dtype=tf.int32),\n",
    "    num_actions=wae_mdp.number_of_discrete_actions,\n",
    "    transition_function=_latent_transition_fn,\n",
    "    epsilon=1e-6)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Time to generate the model: {:.2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c5faa",
   "metadata": {},
   "source": [
    "The CartPole angle and position are always safe (along all episodes): $\\square \\neg \\textsf{Unsafe} \\equiv \\neg \\Diamond \\textsf{Unsafe}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c7d7d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property values: 0.958815\n",
      "Time to compute the values of the property: 445.844 sec\n"
     ]
    }
   ],
   "source": [
    "unsafe_state_test_fn = lambda latent_state: tf.logical_or(\n",
    "    # unsafe position\n",
    "    tf.cast(1. - latent_state[..., 0], tf.bool),\n",
    "    # unsafe angle\n",
    "    tf.cast(1. - latent_state[..., 1], tf.bool))\n",
    "true_fn = lambda latent_state: tf.cast(\n",
    "    tf.ones(shape=(tf.pow(2, wae_mdp.latent_state_size), )),\n",
    "    tf.bool)\n",
    "\n",
    "V = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    T_fn=unsafe_state_test_fn,\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99,)\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'CartPole-v0',)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(1. - latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc92601",
   "metadata": {},
   "source": [
    "## MountainCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e5aceca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': '<wasserstein_mdp.WassersteinMarkovDecisionProcess object at 0x2b12ba1fbee0>', 'state_shape': '(2,)', 'action_shape': '(3,)', 'reward_shape': '(1,)', 'label_shape': '(3,)', 'discretize_action_space': 'False', 'state_encoder_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='state_encoder_network_base')\", 'action_decoder_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='action_decoder_network_base')\", 'transition_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='transition_network_base')\", 'reward_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='reward_network_base')\", 'decoder_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='state_decoder_network_base')\", 'latent_policy_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='discrete_policy_network_base')\", 'steady_state_lipschitz_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='steady_state_network_base')\", 'transition_loss_lipschitz_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='transition_loss_network_base')\", 'latent_state_size': '10', 'number_of_discrete_actions': '16', 'action_encoder_network': \"ModelArchitecture(hidden_units=[512, 512, 512], activation='relu', name='action_encoder_network_base')\", 'state_encoder_pre_processing_network': 'None', 'state_decoder_pre_processing_network': 'None', 'time_stacked_states': 'False', 'state_encoder_temperature': '0.6666666666666666', 'state_prior_temperature': '0.3333333333333333', 'action_encoder_temperature': '0.99', 'latent_policy_temperature': '0.3333333333333333', 'wasserstein_regularizer_scale_factor': 'WassersteinRegularizerScaleFactor(global_scaling=10.0, global_gradient_penalty_multiplier=10.0, steady_state_scaling=100.0, steady_state_gradient_penalty_multiplier=None, local_transition_loss_scaling=25.0, local_transition_loss_gradient_penalty_multiplier=None)', 'encoder_temperature_decay_rate': '0.0', 'prior_temperature_decay_rate': '0.0', 'reset_state_label': 'True', 'autoencoder_optimizer': 'None', 'wasserstein_regularizer_optimizer': 'None', 'entropy_regularizer_scale_factor': '0.0', 'entropy_regularizer_decay_rate': '0.0', 'entropy_regularizer_scale_factor_min_value': '0.0', 'importance_sampling_exponent': '0.4', 'importance_sampling_exponent_growth_rate': '1e-05', 'time_stacked_lstm_units': '128', 'reward_bounds': 'None', 'latent_stationary_network': 'None', 'action_entropy_regularizer_scaling': '0.0', 'enforce_upper_bound': 'False', 'squared_wasserstein': 'True', 'n_critic': '20', 'trainable_prior': 'False', 'state_encoder_type': 'EncodingType.DETERMINISTIC', 'policy_based_decoding': 'False', 'deterministic_state_embedding': 'True', 'state_encoder_softclipping': 'True', 'args': '()', 'kwargs': \"{'evaluation_window_size': 0}\", '__class__': \"<class 'wasserstein_mdp.WassersteinMarkovDecisionProcess'>\", 'eval_policy': '-94.3', 'local_reward_loss': '0.005772148', 'local_transition_loss': '0.42690355', 'training_step': '840000'}\n",
      "Model: \"state_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " state (InputLayer)          [(None, 2)]               0         \n",
      "                                                                 \n",
      " state_encoder_body (Sequent  (None, 510)              525822    \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 6)                 3066      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 528,888\n",
      "Trainable params: 528,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "No action encoder\n",
      "Model: \"autoregressive_transition_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " logistic_layer_input (InputLay  [(None, 13)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " sequential_logistic_distributi  (None, 10)          0           ['logistic_layer_input[0][0]']   \n",
      " on_layer (Sequential)                                                                            \n",
      "                                                                                                  \n",
      " autoregressive_transform (Auto  ((None, 10),        556173      ['sequential_logistic_distributio\n",
      " regressiveTransform)            (None, 10))                     n_layer[0][0]',                  \n",
      "                                                                  'logistic_layer_input[0][0]']   \n",
      "                                                                                                  \n",
      " autoregressive_network_36 (Aut  (None, 10, 1)       556172      []                               \n",
      " oregressiveNetwork)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 556,173\n",
      "Trainable params: 556,172\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"latent_stationary_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " logistic_layer_input (Input  [(None, 0)]              0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " sequential_logistic_distrib  (None, 4)                0         \n",
      " ution_layer (Sequential)                                        \n",
      "                                                                 \n",
      " autoregressive_transform (A  ((None, 4),              529925    \n",
      " utoregressiveTransform)      (None, 4))                         \n",
      "                                                                 \n",
      " autoregressive_network_37 (  (None, 4, 1)             529924    \n",
      " AutoregressiveNetwork)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 529,931\n",
      "Trainable params: 529,924\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "Model: \"latent_policy_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_state (InputLayer)   [(None, 10)]              0         \n",
      "                                                                 \n",
      " discrete_policy_network_bas  (None, 512)              530944    \n",
      " e (Sequential)                                                  \n",
      "                                                                 \n",
      " latent_policy_categorical_l  (None, 3)                1539      \n",
      " ogits (Dense)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532,483\n",
      "Trainable params: 532,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"reward_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 10)]        0           []                               \n",
      "                                                                                                  \n",
      " reward_function_input (Concate  (None, 23)          0           ['latent_state[0][0]',           \n",
      " nate)                                                            'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " reward_network_base (Sequentia  (None, 512)         537600      ['reward_function_input[0][0]']  \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " reward_network_raw_output (Den  (None, 1)           513         ['reward_network_base[0][0]']    \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " reward (Reshape)               (None, 1)            0           ['reward_network_raw_output[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 538,113\n",
      "Trainable params: 538,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"state_reconstruction_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " next_latent_state (InputLay  [(None, 10)]             0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " state_decoder_network_base   (None, 512)              530944    \n",
      " (Sequential)                                                    \n",
      "                                                                 \n",
      " state_decoder (Sequential)  (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,970\n",
      "Trainable params: 531,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"steady_state_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 10)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 23)           0           ['latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " steady_state_network_base (Seq  (None, 512)         537600      ['concatenate_36[0][0]']         \n",
      " uential)                                                                                         \n",
      "                                                                                                  \n",
      " steady_state_lipschitz_network  (None, 1)           513         ['steady_state_network_base[0][0]\n",
      " _output (Dense)                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 538,113\n",
      "Trainable params: 538,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"transition_loss_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " state (InputLayer)             [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_state (InputLayer)      [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 10)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 25)           0           ['state[0][0]',                  \n",
      "                                                                  'action[0][0]',                 \n",
      "                                                                  'latent_state[0][0]',           \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " transition_loss_network_base (  (None, 512)         538624      ['concatenate_37[0][0]']         \n",
      " Sequential)                                                                                      \n",
      "                                                                                                  \n",
      " transition_loss_lipschitz_netw  (None, 1)           513         ['transition_loss_network_base[0]\n",
      " ork_output (Dense)                                              [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 539,137\n",
      "Trainable params: 539,137\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WAE-MDP loaded\n"
     ]
    }
   ],
   "source": [
    "wae_model_path = 'saved_models/experiments/MountainCar-v0/model/'\n",
    "\n",
    "with open(os.path.join(wae_model_path, 'model_infos.json'), 'r') as f:\n",
    "    wae_data = json.load(f)\n",
    "    print(wae_data)\n",
    "\n",
    "wae_mdp = wasserstein_mdp.load(wae_model_path)\n",
    "\n",
    "print(\"WAE-MDP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc376450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE-MDP at training step 840000\n",
      "Size of the latent state space: 1024\n",
      "Local reward loss: 0.00577215\n",
      "Local transition loss: 0.426904\n"
     ]
    }
   ],
   "source": [
    "print(\"WAE-MDP at training step {:d}\".format(eval(wae_data['training_step'])))\n",
    "print(\"Size of the latent state space: {:d}\".format(2 ** wae_mdp.latent_state_size))\n",
    "print('Local reward loss: {:.6g}'.format(eval(wae_data['local_reward_loss'])))\n",
    "print('Local transition loss: {:.6g}'.format(eval(wae_data['local_transition_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "887f277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition/reward model generation\n",
      "Time to generate the model: 9.1 sec\n"
     ]
    }
   ],
   "source": [
    "with suite_gym.load('MountainCar-v0') as py_env:\n",
    "    py_env.reset()\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    original_state = tf_env.current_time_step().observation\n",
    "\n",
    "print('Transition/reward model generation')\n",
    "start = time.time()\n",
    "\n",
    "_latent_transition_fn = lambda latent_state, latent_action: \\\n",
    "        wae_mdp.discrete_latent_transition(\n",
    "            tf.cast(latent_state, tf.float32),\n",
    "            tf.cast(latent_action, tf.float32))\n",
    "\n",
    "latent_transition_fn = model.TransitionFunctionCopy(\n",
    "    num_states=tf.cast(tf.pow(2, wae_mdp.latent_state_size), dtype=tf.int32),\n",
    "    num_actions=wae_mdp.number_of_discrete_actions,\n",
    "    transition_function=_latent_transition_fn,\n",
    "    epsilon=1e-6)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Time to generate the model: {:.2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84e937",
   "metadata": {},
   "source": [
    "The car reaches the flag infinitly often: $\\Box \\Diamond \\textsf{Flag} \\equiv \\neg \\Diamond \\neg (\\Diamond \\textsf{Flag}) = \\neg \\Diamond (\\Box \\neg \\textsf{Flag})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "de3569b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property values: 0.893684\n",
      "Time to compute the values of the property: 15.3366 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "flag_fn = lambda latent_state: tf.cast(latent_state[..., 0], tf.bool)\n",
    "reset_state_fn = lambda latent_state: is_reset_state(latent_state, wae_mdp.atomic_prop_dims)\n",
    "true_fn = lambda latent_state: tf.cast(\n",
    "    tf.ones(shape=(tf.pow(2, wae_mdp.latent_state_size), )),\n",
    "    tf.bool)\n",
    "\n",
    "# always operator (always not Flag)\n",
    "V_1 = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    T_fn=flag_fn,\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.9999,)\n",
    "V_1 = 1. - V_1\n",
    "\n",
    "# eventually operator (eventually not flag)\n",
    "V_2 = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    T_fn=lambda latent_state: tf.logical_not(flag_fn(latent_state)),\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.9999,\n",
    "    transition_to_T_reward=V_1)\n",
    "\n",
    "# eventually not flag, and then always not flag\n",
    "# negation\n",
    "V_3 = 1. - V_2\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'MountainCar-v0',)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V_3\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1111293",
   "metadata": {},
   "source": [
    "## Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b202e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': '<wasserstein_mdp.WassersteinMarkovDecisionProcess object at 0x2ba6945ccf10>', 'state_shape': '(3,)', 'action_shape': '(1,)', 'reward_shape': '(1,)', 'label_shape': '(4,)', 'discretize_action_space': 'True', 'state_encoder_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='state_encoder_network_base')\", 'action_decoder_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='action_decoder_network_base')\", 'transition_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='transition_network_base')\", 'reward_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='reward_network_base')\", 'decoder_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='state_decoder_network_base')\", 'latent_policy_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='discrete_policy_network_base')\", 'steady_state_lipschitz_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='steady_state_network_base')\", 'transition_loss_lipschitz_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='transition_loss_network_base')\", 'latent_state_size': '13', 'number_of_discrete_actions': '3', 'action_encoder_network': \"ModelArchitecture(hidden_units=[256, 256, 256], activation='relu', name='action_encoder_network_base')\", 'state_encoder_pre_processing_network': 'None', 'state_decoder_pre_processing_network': 'None', 'time_stacked_states': 'False', 'state_encoder_temperature': '0.6666666666666666', 'state_prior_temperature': '0.6666666666666666', 'action_encoder_temperature': '0.3333333333333333', 'latent_policy_temperature': '0.5', 'wasserstein_regularizer_scale_factor': 'WassersteinRegularizerScaleFactor(global_scaling=10.0, global_gradient_penalty_multiplier=10.0, steady_state_scaling=25.0, steady_state_gradient_penalty_multiplier=None, local_transition_loss_scaling=25.0, local_transition_loss_gradient_penalty_multiplier=None)', 'encoder_temperature_decay_rate': '0.0', 'prior_temperature_decay_rate': '0.0', 'reset_state_label': 'True', 'autoencoder_optimizer': 'None', 'wasserstein_regularizer_optimizer': 'None', 'entropy_regularizer_scale_factor': '0.0', 'entropy_regularizer_decay_rate': '0.0', 'entropy_regularizer_scale_factor_min_value': '0.0', 'importance_sampling_exponent': '0.4', 'importance_sampling_exponent_growth_rate': '7e-05', 'time_stacked_lstm_units': '128', 'reward_bounds': 'None', 'latent_stationary_network': 'None', 'action_entropy_regularizer_scaling': '0.0', 'enforce_upper_bound': 'False', 'squared_wasserstein': 'True', 'n_critic': '5', 'trainable_prior': 'False', 'state_encoder_type': 'EncodingType.DETERMINISTIC', 'policy_based_decoding': 'False', 'deterministic_state_embedding': 'True', 'state_encoder_softclipping': 'True', 'args': '()', 'kwargs': \"{'evaluation_window_size': 0}\", '__class__': \"<class 'wasserstein_mdp.WassersteinMarkovDecisionProcess'>\", 'eval_policy': '-107.5308', 'local_reward_loss': '0.026674531', 'local_transition_loss': '0.5395084', 'training_step': '370000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 17:53:07.911364: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"state_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " state (InputLayer)          [(None, 3)]               0         \n",
      "                                                                 \n",
      " state_encoder_body (Sequent  (None, 256)              132608    \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,664\n",
      "Trainable params: 134,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"action_encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " action_encoder_input (Concaten  (None, 14)          0           ['latent_state[0][0]',           \n",
      " ate)                                                             'action[0][0]']                 \n",
      "                                                                                                  \n",
      " action_encoder_network_base (S  (None, 256)         135424      ['action_encoder_input[0][0]']   \n",
      " equential)                                                                                       \n",
      "                                                                                                  \n",
      " action_encoder_categorical_log  (None, 3)           771         ['action_encoder_network_base[0][\n",
      " its (Dense)                                                     0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,195\n",
      "Trainable params: 136,195\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"autoregressive_transition_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " logistic_layer_input (InputLay  [(None, 16)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " sequential_logistic_distributi  (None, 13)          0           ['logistic_layer_input[0][0]']   \n",
      " on_layer (Sequential)                                                                            \n",
      "                                                                                                  \n",
      " autoregressive_transform (Auto  ((None, 13),        151006      ['sequential_logistic_distributio\n",
      " regressiveTransform)            (None, 13))                     n_layer[0][0]',                  \n",
      "                                                                  'logistic_layer_input[0][0]']   \n",
      "                                                                                                  \n",
      " autoregressive_network (Autore  (None, 13, 1)       151005      []                               \n",
      " gressiveNetwork)                                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 151,006\n",
      "Trainable params: 151,005\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"latent_stationary_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " logistic_layer_input (Input  [(None, 0)]              0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " sequential_logistic_distrib  (None, 5)                0         \n",
      " ution_layer (Sequential)                                        \n",
      "                                                                 \n",
      " autoregressive_transform (A  ((None, 5),              134406    \n",
      " utoregressiveTransform)      (None, 5))                         \n",
      "                                                                 \n",
      " autoregressive_network_1 (A  (None, 5, 1)             134405    \n",
      " utoregressiveNetwork)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,414\n",
      "Trainable params: 134,405\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n",
      "Model: \"latent_policy_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_state (InputLayer)   [(None, 13)]              0         \n",
      "                                                                 \n",
      " discrete_policy_network_bas  (None, 256)              135168    \n",
      " e (Sequential)                                                  \n",
      "                                                                 \n",
      " latent_policy_categorical_l  (None, 3)                771       \n",
      " ogits (Dense)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,939\n",
      "Trainable params: 135,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"reward_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 13)]        0           []                               \n",
      "                                                                                                  \n",
      " reward_function_input (Concate  (None, 29)          0           ['latent_state[0][0]',           \n",
      " nate)                                                            'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " reward_network_base (Sequentia  (None, 256)         139264      ['reward_function_input[0][0]']  \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " reward_network_raw_output (Den  (None, 1)           257         ['reward_network_base[0][0]']    \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " reward (Reshape)               (None, 1)            0           ['reward_network_raw_output[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 139,521\n",
      "Trainable params: 139,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"state_reconstruction_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " next_latent_state (InputLay  [(None, 13)]             0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " state_decoder_network_base   (None, 256)              135168    \n",
      " (Sequential)                                                    \n",
      "                                                                 \n",
      " state_decoder (Sequential)  (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,939\n",
      "Trainable params: 135,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"action_reconstruction_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " action_reconstruction_input (C  (None, 16)          0           ['latent_state[0][0]',           \n",
      " oncatenate)                                                      'latent_action[0][0]']          \n",
      "                                                                                                  \n",
      " action_decoder_network_base (S  (None, 256)         135936      ['action_reconstruction_input[0][\n",
      " equential)                                                      0]']                             \n",
      "                                                                                                  \n",
      " action_reconstruction_network_  (None, 1)           257         ['action_decoder_network_base[0][\n",
      " raw_output (Dense)                                              0]']                             \n",
      "                                                                                                  \n",
      " action_reconstruction_network_  (None, 1)           0           ['action_reconstruction_network_r\n",
      " output (Reshape)                                                aw_output[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,193\n",
      "Trainable params: 136,193\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"steady_state_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 13)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " steady_state_network_base (Seq  (None, 256)         139264      ['concatenate[0][0]']            \n",
      " uential)                                                                                         \n",
      "                                                                                                  \n",
      " steady_state_lipschitz_network  (None, 1)           257         ['steady_state_network_base[0][0]\n",
      " _output (Dense)                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 139,521\n",
      "Trainable params: 139,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"transition_loss_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " state (InputLayer)             [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_state (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 13)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 33)           0           ['state[0][0]',                  \n",
      "                                                                  'action[0][0]',                 \n",
      "                                                                  'latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " transition_loss_network_base (  (None, 256)         140288      ['concatenate_1[0][0]']          \n",
      " Sequential)                                                                                      \n",
      "                                                                                                  \n",
      " transition_loss_lipschitz_netw  (None, 1)           257         ['transition_loss_network_base[0]\n",
      " ork_output (Dense)                                              [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 140,545\n",
      "Trainable params: 140,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WAE-MDP loaded\n"
     ]
    }
   ],
   "source": [
    "wae_model_path = 'saved_models/experiments/PendulumRandomInit-v1/model/'\n",
    "\n",
    "with open(os.path.join(wae_model_path, 'model_infos.json'), 'r') as f:\n",
    "    wae_data = json.load(f)\n",
    "    print(wae_data)\n",
    "\n",
    "wae_mdp = wasserstein_mdp.load(wae_model_path)\n",
    "\n",
    "print(\"WAE-MDP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac828d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE-MDP at training step 370000\n",
      "Size of the latent state space: 8192\n",
      "Size of the latent action space: 3\n",
      "Local reward loss: 0.0266745\n",
      "Local transition loss: 0.539508\n"
     ]
    }
   ],
   "source": [
    "print(\"WAE-MDP at training step {:d}\".format(eval(wae_data['training_step'])))\n",
    "print(\"Size of the latent state space: {:d}\".format(2 ** wae_mdp.latent_state_size))\n",
    "print(\"Size of the latent action space: {:d}\".format(wae_mdp.number_of_discrete_actions))\n",
    "print('Local reward loss: {:.6g}'.format(eval(wae_data['local_reward_loss'])))\n",
    "print('Local transition loss: {:.6g}'.format(eval(wae_data['local_transition_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16de8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local reward loss: 0.027\n",
      "Local transition loss: 0.54\n",
      "Transition/reward model generation\n",
      "Time to generate the model: 217.49 sec\n"
     ]
    }
   ],
   "source": [
    "with suite_gym.load('Pendulum-v1') as py_env:\n",
    "    py_env.reset()\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    original_state = tf_env.current_time_step().observation\n",
    "\n",
    "_latent_transition_fn = lambda latent_state, latent_action: \\\n",
    "        wae_mdp.discrete_latent_transition(\n",
    "            tf.cast(latent_state, tf.float32),\n",
    "            tf.cast(latent_action, tf.float32))\n",
    "\n",
    "print('Local reward loss: {:.2g}'.format(eval(wae_data['local_reward_loss'])))\n",
    "print('Local transition loss: {:.2g}'.format(eval(wae_data['local_transition_loss'])))\n",
    "\n",
    "print('Transition/reward model generation')\n",
    "#  write the transition/reward functions to tensors,\n",
    "#  to formally check the values in an efficient way\n",
    "start = time.time()\n",
    "\n",
    "latent_transition_fn = model.TransitionFunctionCopy(\n",
    "    num_states=tf.cast(tf.pow(2, wae_mdp.latent_state_size), dtype=tf.int32),\n",
    "    num_actions=wae_mdp.number_of_discrete_actions,\n",
    "    transition_function=_latent_transition_fn,\n",
    "    epsilon=1e-9)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Time to generate the model: {:.2f} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2d12b",
   "metadata": {},
   "source": [
    "There is an episode where the pendulum straightens and then remains straight until the end of the episode: $\\Diamond [\\textsf{Upright}\\, \\mathcal{U} \\textsf{Reset}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09787979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property values: 0.948295\n",
      "Time to compute the values of the property: 658.447 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "upright_test_fn = lambda latent_state: tf.cast(latent_state[..., 0], tf.bool)\n",
    "reset_state_test_fn = lambda latent_state: is_reset_state(latent_state, wae_mdp.atomic_prop_dims)\n",
    "\n",
    "V_1 = reach_C_then_T_values(\n",
    "    C_fn=upright_test_fn,\n",
    "    T_fn=reset_state_test_fn,\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.9999)\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'Pendulum-v1',)\n",
    "# take into account the added absorbing state\n",
    "p_init = tf.concat([p_init, [0.]], axis=-1)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4714fc3",
   "metadata": {},
   "source": [
    "The pendulum always straightens and then remains it until the end of the current episode: $\\Box\\Diamond(\\textsf{Upright}\\mathcal{U}\\textsf{Reset}) \\equiv \\neg \\Diamond \\neg [ \\Diamond(\\textsf{Upright}\\mathcal{U}\\textsf{Reset}) ] \\equiv \\neg \\Diamond \\Box \\neg (\\textsf{Upright}\\mathcal{U}\\textsf{Reset})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3243e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property values: 0.953257\n",
      "Time to compute the values of the property: 697.149 sec\n"
     ]
    }
   ],
   "source": [
    "true_fn = lambda latent_state: tf.cast(\n",
    "    tf.ones(shape=(tf.pow(2, wae_mdp.latent_state_size), )),\n",
    "    tf.bool)\n",
    "\n",
    "# always operator: always not upright until reset\n",
    "# = not eventually upright until reset\n",
    "V_1 = 1 - V\n",
    "\n",
    "# eventually operator: eventually upright and not reset,\n",
    "# then always not (upright until reset)\n",
    "V_2 = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    # cf. negation of not (upright until reset)\n",
    "    T_fn=lambda latent_state: tf.logical_and(\n",
    "        upright_test_fn(latent_state),\n",
    "        tf.logical_not(reset_state_test_fn(latent_state))),\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.9999,\n",
    "    # the reward of transitioning to (absorbing) T is the probability\n",
    "    # of always not (upright until reset), recorded in V_1\n",
    "    transition_to_T_reward=V_1[:-1] \n",
    ")\n",
    "\n",
    "# negation of eventually always not (upright until reset)\n",
    "V_3 = 1 - V_2\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'Pendulum-v1',)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V_3\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe21d4",
   "metadata": {},
   "source": [
    "## LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "11b5eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': '<wasserstein_mdp.WassersteinMarkovDecisionProcess object at 0x2b74bebf5040>', 'state_shape': '(8,)', 'action_shape': '(2,)', 'reward_shape': '(1,)', 'label_shape': '(6,)', 'discretize_action_space': 'True', 'state_encoder_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='state_encoder_network_base')\", 'action_decoder_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='action_decoder_network_base')\", 'transition_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='transition_network_base')\", 'reward_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='reward_network_base')\", 'decoder_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='state_decoder_network_base')\", 'latent_policy_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='discrete_policy_network_base')\", 'steady_state_lipschitz_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='steady_state_network_base')\", 'transition_loss_lipschitz_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='transition_loss_network_base')\", 'latent_state_size': '14', 'number_of_discrete_actions': '3', 'action_encoder_network': \"ModelArchitecture(hidden_units=[256], activation='relu', name='action_encoder_network_base')\", 'state_encoder_pre_processing_network': 'None', 'state_decoder_pre_processing_network': 'None', 'time_stacked_states': 'False', 'state_encoder_temperature': '0.6', 'state_prior_temperature': '0.75', 'action_encoder_temperature': '0.3333333333333333', 'latent_policy_temperature': '0.5', 'wasserstein_regularizer_scale_factor': 'WassersteinRegularizerScaleFactor(global_scaling=10.0, global_gradient_penalty_multiplier=20.0, steady_state_scaling=100.0, steady_state_gradient_penalty_multiplier=None, local_transition_loss_scaling=50.0, local_transition_loss_gradient_penalty_multiplier=None)', 'encoder_temperature_decay_rate': '0.0', 'prior_temperature_decay_rate': '0.0', 'reset_state_label': 'True', 'autoencoder_optimizer': 'None', 'wasserstein_regularizer_optimizer': 'None', 'entropy_regularizer_scale_factor': '0.0', 'entropy_regularizer_decay_rate': '0.0', 'entropy_regularizer_scale_factor_min_value': '0.0', 'importance_sampling_exponent': '0.4', 'importance_sampling_exponent_growth_rate': '1e-05', 'time_stacked_lstm_units': '128', 'reward_bounds': 'None', 'latent_stationary_network': 'None', 'action_entropy_regularizer_scaling': '0.0', 'enforce_upper_bound': 'False', 'squared_wasserstein': 'True', 'n_critic': '15', 'trainable_prior': 'False', 'state_encoder_type': 'EncodingType.DETERMINISTIC', 'policy_based_decoding': 'False', 'deterministic_state_embedding': 'True', 'state_encoder_softclipping': 'False', 'args': '()', 'kwargs': \"{'evaluation_window_size': 0}\", '__class__': \"<class 'wasserstein_mdp.WassersteinMarkovDecisionProcess'>\", 'eval_policy': '282.56876', 'local_reward_loss': '0.020720486', 'local_transition_loss': '0.13135736', 'training_step': '320000'}\n",
      "Model: \"state_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " state (InputLayer)          [(None, 8)]               0         \n",
      "                                                                 \n",
      " state_encoder_body (Sequent  (None, 252)              2268      \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 7)                 1771      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,039\n",
      "Trainable params: 4,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"action_encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " action_encoder_input (Concaten  (None, 16)          0           ['latent_state[0][0]',           \n",
      " ate)                                                             'action[0][0]']                 \n",
      "                                                                                                  \n",
      " action_encoder_network_base (S  (None, 256)         4352        ['action_encoder_input[0][0]']   \n",
      " equential)                                                                                       \n",
      "                                                                                                  \n",
      " action_encoder_categorical_log  (None, 3)           771         ['action_encoder_network_base[0][\n",
      " its (Dense)                                                     0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,123\n",
      "Trainable params: 5,123\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"autoregressive_transition_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " logistic_layer_input (InputLay  [(None, 17)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " sequential_logistic_distributi  (None, 14)          0           ['logistic_layer_input[0][0]']   \n",
      " on_layer (Sequential)                                                                            \n",
      "                                                                                                  \n",
      " autoregressive_transform (Auto  ((None, 14),        12029       ['sequential_logistic_distributio\n",
      " regressiveTransform)            (None, 14))                     n_layer[0][0]',                  \n",
      "                                                                  'logistic_layer_input[0][0]']   \n",
      "                                                                                                  \n",
      " autoregressive_network_40 (Aut  (None, 14, 1)       12028       []                               \n",
      " oregressiveNetwork)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,029\n",
      "Trainable params: 12,028\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"latent_stationary_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " logistic_layer_input (Input  [(None, 0)]              0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " sequential_logistic_distrib  (None, 7)                0         \n",
      " ution_layer (Sequential)                                        \n",
      "                                                                 \n",
      " autoregressive_transform (A  ((None, 7),              3848      \n",
      " utoregressiveTransform)      (None, 7))                         \n",
      "                                                                 \n",
      " autoregressive_network_41 (  (None, 7, 1)             3847      \n",
      " AutoregressiveNetwork)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,855\n",
      "Trainable params: 3,847\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n",
      "Model: \"latent_policy_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_state (InputLayer)   [(None, 14)]              0         \n",
      "                                                                 \n",
      " discrete_policy_network_bas  (None, 256)              3840      \n",
      " e (Sequential)                                                  \n",
      "                                                                 \n",
      " latent_policy_categorical_l  (None, 3)                771       \n",
      " ogits (Dense)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,611\n",
      "Trainable params: 4,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"reward_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 14)]        0           []                               \n",
      "                                                                                                  \n",
      " reward_function_input (Concate  (None, 31)          0           ['latent_state[0][0]',           \n",
      " nate)                                                            'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " reward_network_base (Sequentia  (None, 256)         8192        ['reward_function_input[0][0]']  \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " reward_network_raw_output (Den  (None, 1)           257         ['reward_network_base[0][0]']    \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " reward (Reshape)               (None, 1)            0           ['reward_network_raw_output[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,449\n",
      "Trainable params: 8,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"state_reconstruction_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " next_latent_state (InputLay  [(None, 14)]             0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " state_decoder_network_base   (None, 256)              3840      \n",
      " (Sequential)                                                    \n",
      "                                                                 \n",
      " state_decoder (Sequential)  (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,896\n",
      "Trainable params: 5,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"action_reconstruction_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " action_reconstruction_input (C  (None, 17)          0           ['latent_state[0][0]',           \n",
      " oncatenate)                                                      'latent_action[0][0]']          \n",
      "                                                                                                  \n",
      " action_decoder_network_base (S  (None, 256)         4608        ['action_reconstruction_input[0][\n",
      " equential)                                                      0]']                             \n",
      "                                                                                                  \n",
      " action_reconstruction_network_  (None, 2)           514         ['action_decoder_network_base[0][\n",
      " raw_output (Dense)                                              0]']                             \n",
      "                                                                                                  \n",
      " action_reconstruction_network_  (None, 2)           0           ['action_reconstruction_network_r\n",
      " output (Reshape)                                                aw_output[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,122\n",
      "Trainable params: 5,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"steady_state_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " latent_state (InputLayer)      [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 14)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 31)           0           ['latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " steady_state_network_base (Seq  (None, 256)         8192        ['concatenate_40[0][0]']         \n",
      " uential)                                                                                         \n",
      "                                                                                                  \n",
      " steady_state_lipschitz_network  (None, 1)           257         ['steady_state_network_base[0][0]\n",
      " _output (Dense)                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,449\n",
      "Trainable params: 8,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"transition_loss_lipschitz_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " state (InputLayer)             [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " action (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " latent_state (InputLayer)      [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " latent_action (InputLayer)     [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " next_latent_state (InputLayer)  [(None, 14)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 41)           0           ['state[0][0]',                  \n",
      "                                                                  'action[0][0]',                 \n",
      "                                                                  'latent_state[0][0]',           \n",
      "                                                                  'latent_action[0][0]',          \n",
      "                                                                  'next_latent_state[0][0]']      \n",
      "                                                                                                  \n",
      " transition_loss_network_base (  (None, 256)         10752       ['concatenate_41[0][0]']         \n",
      " Sequential)                                                                                      \n",
      "                                                                                                  \n",
      " transition_loss_lipschitz_netw  (None, 1)           257         ['transition_loss_network_base[0]\n",
      " ork_output (Dense)                                              [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,009\n",
      "Trainable params: 11,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WAE-MDP loaded\n"
     ]
    }
   ],
   "source": [
    "wae_model_path = 'saved_models/experiments/LunarLanderContinuous-v2/model/'\n",
    "\n",
    "with open(os.path.join(wae_model_path, 'model_infos.json'), 'r') as f:\n",
    "    wae_data = json.load(f)\n",
    "    print(wae_data)\n",
    "\n",
    "wae_mdp = wasserstein_mdp.load(wae_model_path)\n",
    "\n",
    "print(\"WAE-MDP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9008e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE-MDP at training step 320000\n",
      "Size of the latent state space: 16384\n",
      "Size of the latent action space: 3\n",
      "Local reward loss: 0.0207205\n",
      "Local transition loss: 0.131357\n"
     ]
    }
   ],
   "source": [
    "print(\"WAE-MDP at training step {:d}\".format(eval(wae_data['training_step'])))\n",
    "print(\"Size of the latent state space: {:d}\".format(2 ** wae_mdp.latent_state_size))\n",
    "print(\"Size of the latent action space: {:d}\".format(wae_mdp.number_of_discrete_actions))\n",
    "print('Local reward loss: {:.6g}'.format(eval(wae_data['local_reward_loss'])))\n",
    "print('Local transition loss: {:.6g}'.format(eval(wae_data['local_transition_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3d90cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate the model: 380.14 sec\n"
     ]
    }
   ],
   "source": [
    "with suite_gym.load('LunarLanderContinuous-v2') as py_env:\n",
    "    py_env.reset()\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    original_state = tf_env.current_time_step().observation\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    _latent_transition_fn = lambda latent_state, latent_action: \\\n",
    "        wae_mdp.discrete_latent_transition(\n",
    "            tf.cast(latent_state, tf.float32),\n",
    "            tf.cast(latent_action, tf.float32))\n",
    "    latent_transition_fn = model.TransitionFunctionCopy(\n",
    "        num_states=tf.cast(tf.pow(2, wae_mdp.latent_state_size), dtype=tf.int32),\n",
    "        num_actions=wae_mdp.number_of_discrete_actions,\n",
    "        transition_function=_latent_transition_fn,\n",
    "        epsilon=1e-6)\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to generate the model: {:.2f} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d46737",
   "metadata": {},
   "source": [
    "The lunar lander never crashes along episodes: it never passes from an unsafe landing state to the reset state, i.e., $\\Box [\\neg (\\neg \\textsf{SafeLanding} \\wedge \\bigcirc \\textsf{Reset})] \\equiv \\neg \\Diamond (\\neg \\textsf{SafeLanding} \\wedge \\bigcirc \\textsf{Reset})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "safe_landing_fn = lambda latent_state: tf.logical_and(\n",
    "    tf.cast(latent_state[..., 1], tf.bool),\n",
    "    tf.cast(latent_state[..., 5], tf.bool))\n",
    "unsafe_landing_fn = lambda latent_state: tf.logical_not(safe_landing_fn(latent_state))\n",
    "reset_state_fn = lambda latent_state: is_reset_state(latent_state, wae_mdp.atomic_prop_dims)\n",
    "\n",
    "V = reach_C_then_T_values(\n",
    "    C_fn=unsafe_landing_fn,\n",
    "    T_fn=reset_state_fn,\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99)\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'LunarLanderContinuous-v2',)\n",
    "# take into account the added absorbing state\n",
    "p_init = tf.concat([p_init, [0.]], axis=-1)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(1. - latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e882cb",
   "metadata": {},
   "source": [
    "There is an episode where the angle of the lander is unsafe during the landing:\n",
    "$\\Diamond [\\textsf{Unsafe} \\, \\mathcal{U}\\, \\textsf{LegContact}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0375eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "unsafe_angle_fn = lambda latent_state: tf.cast(latent_state[..., 0], tf.bool)\n",
    "leg_contact_fn = lambda latent_state: tf.cast(latent_state[..., 1], tf.bool)\n",
    "\n",
    "\n",
    "V = reach_C_then_T_values(\n",
    "    C_fn=unsafe_angle_fn,\n",
    "    T_fn=leg_contact_fn,\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99)\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'LunarLanderContinuous-v2',)\n",
    "# take into account the added absorbing state\n",
    "p_init = tf.concat([p_init, [0.]], axis=-1)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee9991",
   "metadata": {},
   "source": [
    "In all episodes, the angle of the lander is safe until it lands: $\\Box (\\textsf{SafeAngle}\\, \\mathcal{U} \\, \\textsf{LegContact}) \\equiv \\neg \\Diamond [(\\textsf{SafeAngle} \\wedge \\neg \\textsf{LegContact}) \\, \\mathcal{U}\\, (\\neg \\textsf{SafeAngle} \\wedge \\neg \\textsf{LegContact}) \\vee \\Box(\\textsf{SafeAngle} \\wedge \\neg \\textsf{LegContact})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd64978",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "safe_angle_fn = lambda latent_state: tf.math.logical_not(tf.cast(latent_state[..., 0], tf.bool))\n",
    "leg_contact_fn = lambda latent_state: tf.cast(latent_state[..., 1], tf.bool)\n",
    "true_fn = lambda latent_state: tf.cast(\n",
    "    tf.ones(shape=(tf.pow(2, wae_mdp.latent_state_size), )),\n",
    "    tf.bool)\n",
    "\n",
    "# until operator\n",
    "# (discounted) probability of being (safe and not leg contact)\n",
    "# until (not safe and not leg contact)\n",
    "V_1 = C_until_T_values(\n",
    "    C_fn=lambda latent_state: tf.logical_and(\n",
    "        safe_angle_fn(latent_state),\n",
    "        tf.logical_not(leg_contact_fn(latent_state))),\n",
    "    T_fn=lambda latent_state: tf.logical_and(\n",
    "        tf.logical_not(safe_angle_fn(latent_state)),\n",
    "        tf.logical_not(leg_contact_fn(latent_state))),\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99)\n",
    "\n",
    "# always operator\n",
    "V_2 = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    T_fn=lambda latent_state: tf.logical_or(\n",
    "        tf.logical_not(safe_angle_fn(latent_state)),\n",
    "        leg_contact_fn(latent_state)),\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99)\n",
    "# discounted probability of always (safe and not leg contact)\n",
    "V_2 = 1. - V_2\n",
    "\n",
    "# OR operator\n",
    "V_3 = tf.reduce_max(\n",
    "    tf.concat([\n",
    "        tf.expand_dims(V_1, -1),\n",
    "        tf.expand_dims(V_2, -1)],\n",
    "        axis=-1),\n",
    "    axis=-1)\n",
    "\n",
    "# eventually operator\n",
    "# eventually reach (safe and not leg contact)\n",
    "V_4 = C_until_T_values(\n",
    "    C_fn=true_fn,\n",
    "    T_fn=lambda latent_state: tf.logical_and(\n",
    "        safe_angle_fn(latent_state),\n",
    "        tf.logical_not(leg_contact_fn(latent_state))),\n",
    "    transition_matrix=latent_transition_fn.to_dense(),\n",
    "    latent_state_size=wae_mdp.latent_state_size,\n",
    "    A=wae_mdp.number_of_discrete_actions,\n",
    "    latent_policy=wae_mdp.get_latent_policy(action_dtype=tf.int64),\n",
    "    gamma=0.99,\n",
    "    # set the rewards of transitioning to (absorbing) T \n",
    "    # to the discounted probabilities of the event recorded in V_3\n",
    "    transition_to_T_reward=V_3\n",
    ")\n",
    "\n",
    "# Final property values\n",
    "V_prop = 1. - V_4\n",
    "\n",
    "# compute the initial distribution\n",
    "p_init = get_p_init(\n",
    "    wae_mdp,\n",
    "    tf_env.current_time_step().observation,\n",
    "    latent_transition_fn,\n",
    "    'LunarLanderContinuous-v2',)\n",
    "\n",
    "# get the values for the initial distribution\n",
    "latent_mdp_values = tf.reduce_sum(\n",
    "    p_init * V_prop\n",
    ") / tf.reduce_sum(p_init)\n",
    "\n",
    "tf.print(\"property values: {:.6g}\".format(latent_mdp_values))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"Time to compute the values of the property: {:2g} sec\".format(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72776ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
